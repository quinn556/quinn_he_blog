[
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "973-800-7640 / 8hequinn8@gmail.com / linkedin.com/in/quinnhe\nEDUCATION\nM.S. in Data Analytics and Computational Social Science Expected May 2023 University of Massachusetts Amherst, Amherst MA GPA: 3.7 Relevant Coursework: Text as Data, Digital Behavioral Data, Research Methods, Quantitative Analysis\nB.A. in English May 2021\nUniversity of Massachusetts Amherst, Amherst MA GPA: 3.2 Minor: Psychology Certificate: Study and Practice of Writing Relevant Coursework: Intro to Professional Writing, Rhetoric, Writing & Society, Behavioral Neuroscience\nSKILLS\n• Programming languages: R (intermediate), SQL (beginner), Python (beginner) • Software: Microsoft Word, PowerPoint, Excel, familiarity with Tableau, Google Suite, Github, Slack • Techniques: Text-as-data, Web scrapping, Quantitative research\nEXPERIENCE\nSales Associate, Guitar Center, Totowa NJ Sept 2021 – June 2022 • Achieved top 3 in sales among coworkers in first 3 months • Provided excellent customer service to wide range of customers • Built rapport with customers leading to repeated sales to same people • Resolved problems independently during transactions\nHead Arts Editor, Massachusetts Daily Collegian, Amherst MA May 2020 – May 2021 • Led weekly meetings online during COVID keeping high writer retention • Managed Arts/Living staff of 20 plus writers • Aided in management and transition of newspaper to remote work during COVID\nEditorial Assistant, GoNOMAD.com, Remote (Deerfield, MA) May 2020 – Sept 2020 • Wrote and edited multiple articles on weekly basis • Edited SEO data on past articles kept on Excel files • Thrived in remote work to consistently produce articles and complete tasks\nInformation Technology Specialist, UMass Amherst Libraries, Amherst, MA June 2019 – Sept 2019 • Aided students with general computer and IT questions • Debugged computers on campus • Independently redesigned Veteran’s Office computer workspace\nNOTABLE PROJECTS\nGun Violence in America • Quantitative research project examining gun violence • Cleaned and analyzed data on incidents of gun violence and location • Created visualizations and variables to compare in what states gun violence occurs most based on policy\nA Study of January 6th and Echo Chambers • Examined tweets and posts from Twitter and Parler from Jan. 1-11 2021 • Collaborated in team based project multi-modally • Hand coded tweets and posts then ran sentiment analysis • Utilized Excel to organize hand coding\nMEMBERSHIP & ACTIVITES\n• Alpha Chi Rho: Executive Board Member, Recruitment Chair\n• Eagle Scout: Awarded 2016"
  },
  {
    "objectID": "posts/Blog_post_5_QH.html",
    "href": "posts/Blog_post_5_QH.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "library(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#thoughts-for-blog-post-5",
    "href": "posts/Blog_post_5_QH.html#thoughts-for-blog-post-5",
    "title": "Blog Post 5",
    "section": "1 Thoughts for Blog Post 5",
    "text": "1 Thoughts for Blog Post 5\n\nSet up individual meeting to go over project and other blog posts\nDictonary comparison is about finding the best dictionary for my data set. Always look at multiple dictionaries\nBlog Post 5 should focus on a more narrative structure to discuss background and what I want to accomplish in the study. Discuss any issues I have been having or start setting up the structure of the final project.\nRender Blog Post 4 as a PDF or html so he doesnt have to run my long, intensive chunks of code.\nShould I implement Naive Bayes classifier?"
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#touch-on",
    "href": "posts/Blog_post_5_QH.html#touch-on",
    "title": "Blog Post 5",
    "section": "2 Touch on",
    "text": "2 Touch on\n\nselection bias on poster\nsentiment analysis\nmaybe combine other right leaning subreddits so I can have enough comments to analyze against the blue comments\nshould I use LDA or structured topic modeling? Maybe both?\nDiscuss why I chose particular things (which models, which dictionaries)"
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#data",
    "href": "posts/Blog_post_5_QH.html#data",
    "title": "Blog Post 5",
    "section": "3 Data",
    "text": "3 Data\nThe data set I am using are a collection of comments from the subreddit /r/democrats and /r/republican over the past year. 11/08/2021 - 11/08/2022. The democrat sub contains 26,764 comments and the republican sub contains 1,535 comments. I may end up recollecting when the final project comes around, but for now this is the data set. The smaller amount of comments comes from the fact /r/republican is a much smaller subreddit than /r/democrats by almost 200,000. This may be an issue as I go forward, but only because I will have way more content to look at with the democrats subreddit over the republican. Topic modelling and sentiment analysis will still be run anyway on what material I have available within each respective subreddit.\nThe comments data sets were scrapped using the RedditExtractoR package. The following process was done to both subreddit comments: extraction with R package, turned into corpus, tokenized, removed stopwords, urls, numbers, and symbols, then created DFMs and FCMs.\nEach entry of the data set, for both subreddits, and the most important variables includes a timestamp, upvotes, downvotes, user, link to comment, and the comment itself. Reddit users do not typically follow one another in the traditional sense, like Facebook and Twitter, but it may be useful to add a semantic network to see if that truly is the case. Do reddit users on one subreddit tend to have a more close knit community?\nSubreddits are forum groups that have a specific topic. You could find a subreddit on many different topics ranging from a variety of interests. Political subreddits will tend to have political discourse with many longer comments.\nI may have to scrape from another right leaning subreddit and combine it with /r/republicans or do a whole other analysis of it because /r/democrats is so large, I think I will want more data from the other side of the political spectrum to compare."
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#research-question",
    "href": "posts/Blog_post_5_QH.html#research-question",
    "title": "Blog Post 5",
    "section": "4 Research Question",
    "text": "4 Research Question\nDo political subreddits tend to have more negative sentiment? - Which subreddit has more hostile/negative speech\nWhat do these subreddits talk about? Do they discuss the same subjects from a different political lens?\nMy goal for this project is to look at both /r/democrats and /r/republican to get a better idea as to how these two communities talk about different issues, both political and not. Reddit is a great platform to research because it is a heavily text based platform with an open API. Twitter would also be useful, but with Reddit, I can easily look into different communities and measure how they communicate and what they talk about."
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#methods",
    "href": "posts/Blog_post_5_QH.html#methods",
    "title": "Blog Post 5",
    "section": "5 Methods",
    "text": "5 Methods\nOnce the comments were loaded into R, I created a corpus and tokenized both subreddits. I then implemented preprocessing techniques removing stopwords, punctuation, numbers, urls, etc. Every political subreddit post contains a comment by the AutoModerator telling people to act civilized or to display a general message about commenting etiquette. Some posts also contain [deleted] users or comments signifying someone deleted their account or comment in the comment thread. I removed both of these in order to not interfere with any topic modeling or sentiment analysis later on in the project.\nNext, I ran the comments through multiple dictionaries to find a good fit for my data set. I found the Lexicoder dictionary will probably fit my data set the best, but I may work with a few more to compare. I then ran topic modeling techniques and created a few visualizations to help get a better look as a whole.\nIn previous blog posts, I created wordclouds of the titles of posts on each respective subreddit. This was before I figured out how to get the comments of posts, which are what I was primarily focused on. The wordclouds only moderately benefited my entire project as they were a quick way to get a larger picture of the types of subjects found in the subreddit. The smaller words on the outside of the wordclouds were the most interesting because I consider them the niche subjects in the subreddits, which I found more useful than their more comment post titles.\nI think the most useful types of analysis for this project will be some LDA modelling, supervised learning, sentiment analysis using 1-2 different dictionaries, and maybe one other analysis. I must make sure I can fit certain visualizations on the poster.\nOnce I choose which dictionaries to use, I think I will try to implement a highcharter graph of sentiment, at least for positive and negative sentiment among subreddits. On the final poster, what I’ll want to have graphed sentiments for both subreddits. I must keep in mind I will need 2 of each type of graph so comparison can be visualized."
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#reason-for-lsd-dictionary-in-sentiment-analysis",
    "href": "posts/Blog_post_5_QH.html#reason-for-lsd-dictionary-in-sentiment-analysis",
    "title": "Blog Post 5",
    "section": "6 Reason for LSD dictionary in Sentiment Analysis",
    "text": "6 Reason for LSD dictionary in Sentiment Analysis\nThe dictionary consists of 2,858 “negative” sentiment words and 1,709 “positive” sentiment words. A further set of 2,860 and 1,721 negations of negative and positive words, respectively, is also included. While many users will find the non-negation sentiment forms of the LSD adequate for sentiment analysis, Young and Soroka (2012) did find a small, but non-negligible increase in performance when accounting for negations. Users wishing to test this or include the negations are encouraged to subtract negated positive words from the count of positive words, and subtract the negated negative words from the negative count."
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#reason-for-moral-foundations-dictionary-in-sentiment-analysis",
    "href": "posts/Blog_post_5_QH.html#reason-for-moral-foundations-dictionary-in-sentiment-analysis",
    "title": "Blog Post 5",
    "section": "7 Reason for Moral Foundations dictionary in Sentiment Analysis",
    "text": "7 Reason for Moral Foundations dictionary in Sentiment Analysis\nA quanteda dictionary object containing the Moral Foundations Dictionary, a publicly available dictionaries with information on the proportions of virtue and vice words for each foundation. The categories are harm (vice/virtue), fairness (vice/virtue), ingroup (vice/virtue), authority (vice/virtue), purity (vice/virtue) and morality (general).\nThis dictionary and LSD may be the best because I can look for postitive and negative sentiment, but MFD contains other interesting measurements I want to apply to the subreddits.\nI may only use these two dictionaries for sentiment analysis, but I can decide what the final one will be in Blog Post 6.\n*The code for the different dictionaries is located in Blog Post 3."
  },
  {
    "objectID": "posts/Blog_post_5_QH.html#notes-for-final-blog-post-6",
    "href": "posts/Blog_post_5_QH.html#notes-for-final-blog-post-6",
    "title": "Blog Post 5",
    "section": "8 Notes for Final Blog Post 6",
    "text": "8 Notes for Final Blog Post 6\n\nAdd a few of the final types of analysis I want\nMore supervised learning and more LDA modelling (try out different K)\nPossibly try causal inference\nFinalize certain models and state why I chose them (LDA, etc. )"
  },
  {
    "objectID": "posts/FinalTextasData.html",
    "href": "posts/FinalTextasData.html",
    "title": "inside /r/democrats and /r/republican",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalTextasData.html#notes-from-other-research",
    "href": "posts/FinalTextasData.html#notes-from-other-research",
    "title": "inside /r/democrats and /r/republican",
    "section": "Notes from other research",
    "text": "Notes from other research"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Quinn He",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome/index.html#notes",
    "href": "posts/welcome/index.html#notes",
    "title": "Quinn He",
    "section": "Notes",
    "text": "Notes\nInclude previous blog posts and other homeworks from my other classes. I can include the 601 final as well"
  },
  {
    "objectID": "posts/Final Part 2.html",
    "href": "posts/Final Part 2.html",
    "title": "Final Part 2 QH",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(mapview)\n#library(summarytools)\nlibrary(GGally)\nlibrary(stargazer)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Final Part 2.html#notes-from-class",
    "href": "posts/Final Part 2.html#notes-from-class",
    "title": "Final Part 2 QH",
    "section": "Notes from class",
    "text": "Notes from class\nWhy did I choose particular interaction terms?\nWhy did I choose the variables I did for certain models? There must be a reason for each different model"
  },
  {
    "objectID": "posts/Final Part 2.html#data-read-in",
    "href": "posts/Final Part 2.html#data-read-in",
    "title": "Final Part 2 QH",
    "section": "Data Read-in",
    "text": "Data Read-in\nHere is the Miami housing dataset I am using for the project.\n\n\nCode\nmiami_housing <- read_csv(\"~/Documents/DACSS Program/data/miami-housing.csv\")\n\n\nRows: 13932 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): LATITUDE, LONGITUDE, PARCELNO, SALE_PRC, LND_SQFOOT, TOT_LVG_AREA,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBelow I read in data I may end up using that relates to the county and state election data. For now, my main data set is miami_housing.\n\n\nCode\ncounty_election <- read_csv(\"~/Documents/DACSS Program/data/president_county.csv\")\n\n\nRows: 4633 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (3): current_votes, total_votes, percent\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nstate_election <- read_csv(\"~/Documents/DACSS Program/data/president_state.csv\")\n\n\nRows: 52 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): state\ndbl (1): total_votes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\npresident_county <- read_csv(\"~/Documents/DACSS Program/data/president_county_candidate.csv\")\n\n\nRows: 32177 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): state, county, candidate, party\ndbl (1): total_votes\nlgl (1): won\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/Final Part 2.html#information-about-data-set",
    "href": "posts/Final Part 2.html#information-about-data-set",
    "title": "Final Part 2 QH",
    "section": "Information about data set",
    "text": "Information about data set\nThe dataset contains information on 13,932 single-family homes sold in Miami .\nNames before column rename: PARCELNO: unique identifier for each property. About 1% appear multiple times. SALE_PRC: sale price (\\() LND_SQFOOT: land area (square feet) TOTLVGAREA: floor area (square feet) SPECFEATVAL: value of special features (e.g., swimming pools) (\\)) RAIL_DIST: distance to the nearest rail line (an indicator of noise) (feet) OCEAN_DIST: distance to the ocean (feet) WATER_DIST: distance to the nearest body of water (feet) CNTR_DIST: distance to the Miami central business district (feet) SUBCNTR_DI: distance to the nearest subcenter (feet) HWY_DIST: distance to the nearest highway (an indicator of noise) (feet) age: age of the structure avno60plus: dummy variable for airplane noise exceeding an acceptable level structure_quality: quality of the structure month_sold: sale month in 2016 (1 = jan) LATITUDE LONGITUDE\nI first want to rename some columns because I am not a fan of the format of the stock column names,\n\n\nCode\nmiami_housing <- miami_housing %>% \n  rename(\"latitude\" = \"LATITUDE\",\n         \"longitude\" = \"LONGITUDE\",\n         \"sale_price\" = \"SALE_PRC\",  \n         \"land_sqfoot\" = \"LND_SQFOOT\",  \n         \"floor_sqfoot\" = \"TOT_LVG_AREA\",\n         \"special_features\" = \"SPEC_FEAT_VAL\",  \n         \"dist_2_rail\" = \"RAIL_DIST\",  \n         \"dist_2_ocean\" = \"OCEAN_DIST\", \n         \"dist_2_nearest_water\" = \"WATER_DIST\",  \n         \"dist_2_biz_center\" = \"CNTR_DIST\",  \n         \"dis_2_nearest_subcenter\"= \"SUBCNTR_DI\", \n         \"dist_2_hiway\" = \"HWY_DIST\", #close distance is a negative trait \n         \"home_age\" = \"age\") \n\n\nI wonder if it would be worth creating another column called “county” based off of the longitude and latitude coordinates. This would make some graphs more interesting since I could fill by county in a ggplot graph."
  },
  {
    "objectID": "posts/Final Part 2.html#research-question",
    "href": "posts/Final Part 2.html#research-question",
    "title": "Final Part 2 QH",
    "section": "Research Question",
    "text": "Research Question\nDoes the saying “location, location, location” really matter when it comes to housing prices in Miami or are there other factors as well that contribute to price?"
  },
  {
    "objectID": "posts/Final Part 2.html#hypothesis",
    "href": "posts/Final Part 2.html#hypothesis",
    "title": "Final Part 2 QH",
    "section": "Hypothesis",
    "text": "Hypothesis\nOutcome variable: Sale price of houses in Miami\nExplanatory variable: Distance from the ocean, measured in feet\nHypothesis: Houses closer (lower dist_2_ocean) will have a higher price than houses farther."
  },
  {
    "objectID": "posts/Final Part 2.html#descriptive-statistics",
    "href": "posts/Final Part 2.html#descriptive-statistics",
    "title": "Final Part 2 QH",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI’ll have to go further in depth into this research that I found.\nAccording to Redfin, the median sale price for houses in Miami is $530,000. https://www.redfin.com/city/11458/FL/Miami/housing-market\nhttps://www.proquest.com/docview/222418064?pq-origsite=gscholar&fromopenview=true - This study found that increase in house prices from 2003-2004 was largely due to interest rates, housing construction, unemployment, and household income.\nhttps://link.springer.com/article/10.1023/A:1007751112669 - uses OLS to predict house prices - The problem with the above procedure has to do with neighborhood effects. Every realtor knows that location is an extremely important determinant of the price of a house, and yet incorporating neighborhood effects into an ordinary regression model is problematic, for two reasons: (1) neighborhood quality is an unobservable variable, and (2) the measurement of neighborhood quality ( presuming that it can be measured) requires the knowledge of neighborhood boundaries. The ®rst issue can be resolved by using proxies for neighborhood quality. Variables such as crime rates, school quality measures (such as scores on standardized tests),3 and socioeconomic characteristics (such as race and income) have all been used as proxies for neighborhood quality. - The data used in the analysis come from 1978 multiple listings for Baltimore, Maryland. - The original data set consists of 2157 house sales. Of these, 664 records contained missing values, leaving 1493 valid observations. The valid records were randomly divided into two groupsÐan estimation sample and a prediction sample. The estimation sample contains 1000 observations, and the prediction sample contains 493 observations. -\nhttps://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4932.2005.00243.x?casa_token=Lk-oinHQfCoAAAAA:3YFA9GRG6r9UwIJa8-8Z40x77ENRHm07d9CQ7dLdRZD5VoGjSVB4hUUcKU8yWiJapg0csSONiwxZzqWJ\nhttps://www.sciencedirect.com/science/article/pii/S1051137712000228?casa_token=DwJ93qbzqLAAAAAA:8vClfNoxs09D_UL4BCg-Ds36vurfjk8t0uK6Hft50ytFeWo3-XaFlsj5r0WBW4lB1jGqgoaqwXA\nhttps://link.springer.com/article/10.1007/s11146-007-9053-7\nhttps://eprints.gla.ac.uk/221903/1/221903.pdf"
  },
  {
    "objectID": "posts/Final Part 2.html#exploratory-analysis",
    "href": "posts/Final Part 2.html#exploratory-analysis",
    "title": "Final Part 2 QH",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\n\nCode\nstr(miami_housing)\n\n\nspec_tbl_df [13,932 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ latitude               : num [1:13932] 25.9 25.9 25.9 25.9 25.9 ...\n $ longitude              : num [1:13932] -80.2 -80.2 -80.2 -80.2 -80.2 ...\n $ PARCELNO               : num [1:13932] 6.22e+11 6.22e+11 6.22e+11 6.22e+11 6.22e+11 ...\n $ sale_price             : num [1:13932] 440000 349000 800000 988000 755000 630000 1020000 850000 250000 1220000 ...\n $ land_sqfoot            : num [1:13932] 9375 9375 9375 12450 12800 ...\n $ floor_sqfoot           : num [1:13932] 1753 1715 2276 2058 1684 ...\n $ special_features       : num [1:13932] 0 0 49206 10033 16681 ...\n $ dist_2_rail            : num [1:13932] 2816 4359 4413 4585 4063 ...\n $ dist_2_ocean           : num [1:13932] 12811 10648 10574 10156 10837 ...\n $ dist_2_nearest_water   : num [1:13932] 348 338 297 0 327 ...\n $ dist_2_biz_center      : num [1:13932] 42815 43505 43530 43798 43600 ...\n $ dis_2_nearest_subcenter: num [1:13932] 37742 37340 37329 37423 37551 ...\n $ dist_2_hiway           : num [1:13932] 15955 18125 18200 18514 17903 ...\n $ home_age               : num [1:13932] 67 63 61 63 42 41 63 21 56 63 ...\n $ avno60plus             : num [1:13932] 0 0 0 0 0 0 0 0 0 0 ...\n $ month_sold             : num [1:13932] 8 9 2 9 7 2 2 9 3 11 ...\n $ structure_quality      : num [1:13932] 4 4 4 4 4 4 5 4 4 5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   LATITUDE = col_double(),\n  ..   LONGITUDE = col_double(),\n  ..   PARCELNO = col_double(),\n  ..   SALE_PRC = col_double(),\n  ..   LND_SQFOOT = col_double(),\n  ..   TOT_LVG_AREA = col_double(),\n  ..   SPEC_FEAT_VAL = col_double(),\n  ..   RAIL_DIST = col_double(),\n  ..   OCEAN_DIST = col_double(),\n  ..   WATER_DIST = col_double(),\n  ..   CNTR_DIST = col_double(),\n  ..   SUBCNTR_DI = col_double(),\n  ..   HWY_DIST = col_double(),\n  ..   age = col_double(),\n  ..   avno60plus = col_double(),\n  ..   month_sold = col_double(),\n  ..   structure_quality = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\n\nCode\nsummary(miami_housing)\n\n\n    latitude       longitude         PARCELNO           sale_price     \n Min.   :25.43   Min.   :-80.54   Min.   :1.020e+11   Min.   :  72000  \n 1st Qu.:25.62   1st Qu.:-80.40   1st Qu.:1.079e+12   1st Qu.: 235000  \n Median :25.73   Median :-80.34   Median :3.040e+12   Median : 310000  \n Mean   :25.73   Mean   :-80.33   Mean   :2.356e+12   Mean   : 399942  \n 3rd Qu.:25.85   3rd Qu.:-80.26   3rd Qu.:3.060e+12   3rd Qu.: 428000  \n Max.   :25.97   Max.   :-80.12   Max.   :3.660e+12   Max.   :2650000  \n  land_sqfoot     floor_sqfoot  special_features  dist_2_rail     \n Min.   : 1248   Min.   : 854   Min.   :     0   Min.   :   10.5  \n 1st Qu.: 5400   1st Qu.:1470   1st Qu.:   810   1st Qu.: 3299.4  \n Median : 7500   Median :1878   Median :  2766   Median : 7106.3  \n Mean   : 8621   Mean   :2058   Mean   :  9562   Mean   : 8348.5  \n 3rd Qu.: 9126   3rd Qu.:2471   3rd Qu.: 12352   3rd Qu.:12102.6  \n Max.   :57064   Max.   :6287   Max.   :175020   Max.   :29621.5  \n  dist_2_ocean     dist_2_nearest_water dist_2_biz_center\n Min.   :  236.1   Min.   :    0        Min.   :  3826   \n 1st Qu.:18079.3   1st Qu.: 2676        1st Qu.: 42823   \n Median :28541.8   Median : 6923        Median : 65852   \n Mean   :31691.0   Mean   :11960        Mean   : 68490   \n 3rd Qu.:44310.7   3rd Qu.:19200        3rd Qu.: 89358   \n Max.   :75744.9   Max.   :50400        Max.   :159976   \n dis_2_nearest_subcenter  dist_2_hiway        home_age       avno60plus     \n Min.   :  1463          Min.   :   90.2   Min.   : 0.00   Min.   :0.00000  \n 1st Qu.: 23996          1st Qu.: 2998.1   1st Qu.:14.00   1st Qu.:0.00000  \n Median : 41110          Median : 6159.8   Median :26.00   Median :0.00000  \n Mean   : 41115          Mean   : 7723.8   Mean   :30.67   Mean   :0.01493  \n 3rd Qu.: 53949          3rd Qu.:10854.2   3rd Qu.:46.00   3rd Qu.:0.00000  \n Max.   :110554          Max.   :48167.3   Max.   :96.00   Max.   :1.00000  \n   month_sold     structure_quality\n Min.   : 1.000   Min.   :1.000    \n 1st Qu.: 4.000   1st Qu.:2.000    \n Median : 7.000   Median :4.000    \n Mean   : 6.656   Mean   :3.514    \n 3rd Qu.: 9.000   3rd Qu.:4.000    \n Max.   :12.000   Max.   :5.000    \n\n\nAt a glance, I do not care about summary statistics for longitude and latitude. Sale price is worth noting with a minimum of $72,000 and a max of $2.6 million. There are some pretty old homes in the data set with the most at almost 100 years old. A minimum age of zero may be something to watch out for because it is not clear as to if the home is less than 1 year old. I must also find out how the distance is measured. I believe distance in this data set is measured in feet so we can see there may be a few water front properties that I will guess fetch a high sale price. Homes farther away may be cheaper if they are not located near another desirable location.\nWhile typically a negative, the distance to highway can be viewed as a positive trait to a home. For the sake of this analysis and to go with typical association with close proximity to a highway, I will view closer distance as a negative.\nAnother note, is I will have one linear model, m1, to act as the model with all of the variables in it. I want it as a baseline model to see the factor all of the variables play in house price. Since multicollinearity may be a factor, I will not use this model too much.\n\n\nCode\nm1 <- lm(sale_price ~ ., data = miami_housing)"
  },
  {
    "objectID": "posts/Final Project 601.html",
    "href": "posts/Final Project 601.html",
    "title": "Final 601",
    "section": "",
    "text": "Quinn He\nDACSS 601\nDecember 22, 2021\nIntroduction:\nGun violence plagues the United States. In the past two to three decades, gun violence has been on a steady rise, a rise that has not faltered. With another school shooting happening late last month, I thought it best to visualize and get familiar with the rise of school shootings, where they most tend to happen, at what frequency, and how deadly they are. The problem of gun violence is so multifaceted, bogged down by political interests and personal ideology. In one paper, I do not aim to address the problem of gun violence in America, but I do hope to help visualize certain patterns in location and frequency that aid in discussion of this national public safety issue.\nBased on anecdotal evidence, I find Colorado has a large amount of at least infamous shootings, but this data set will help prove if I am right or if other states share the burden. Columbine, Parkland, Sandy Hook, are all names that society has felt one way or another. In years to come, names will continue to pile onto this never ending list if actions to curb gun violence are not implemented.\nGun rights and gun violence attract intense debates and furious supporters on both sides of this never-ending issue. For an issue as divisive as this, there needs to be clear visualization of where and how frequent shootings occur. The goal of this project is to focus on the areas most impacted by gun violence. Where do many of these incidents occur? Is there a period of time when the frequency of gun violence was higher than others? Further areas the project could continue to and limitations will be discussed in the conclusion.\nData:\nThere were only a handful of other datasets on Kaggle that relate to gun violence, but I chose US Gun Violence because of how extensive it was with variables that suited what I wanted to look at. The dataset was aggregated from the gun violence archive website (www.gunviolencearchive.org) and contains incidents of gun violence from January 2014 to September 2021. While some of the entries are notorious mass shootings, many are incidents with only a few injured.\nWith shootings that may have consumed news cycles for weeks, imbedding themselves in US culture, to gun violence that may have only snapped onto a newsfeed, this dataset contains it all. Hopefully, this paints a picture of a highly diverse dataset with all kinds of incidents of gun violence. Some states in the graphs will be compared based on if they are shall issue state or a may issue state. Shall issue refers to states that will issue a gun permit as long as you pass basic requirements set up by the local law. A may issue state will issue a license at the discretion of the local law enforcement. Essential, a may issue is not a guarantee of a permit, while a shall issue is.\n\ngun_violence <- read_csv(\"US-Gun-Violence.csv\")\n\nRows: 3230 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): incident_date, state, city_or_county, address\ndbl (3): incident_id, killed, injured\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nview(gun_violence)\n\nAbove is the data set in its entirety. As you can see it’s laid out with seven columns. The following variables are included in this set: “incident_id”, “incident_date”, “state”, “city_or_county”, “address”, “killed”, “injured”. The first column, “incident_id”, is irrelevant to this research so I will get rid of it in the next section. The same goes for the “address” column.\nThe dataset is quite clean without essentially any missing values or entries. This is one of the cleaner datasets I discovered. Unlike government or self collected datasets that can be messy or poorly formatted, I found this one to be well organized and thorough. It became the obvious choice when looking at other datasets when I was looking at the cleanliness and organization of the Kaggle page. As a first project of this scale, my main priority was to find a dataset that was not too large in scope, but one I could still mold to my liking and have substantial data entries. US Gun Violence fits all that criteria.\nFor right now, I just want to create variables for certain chunks of data and create some variables.\nMutating Data:\nMost Violent Shootings: 2014 - 2021:\n\nmost_violent <- filter(gun_violence, injured > 10, killed > 10)\n\n\nview(most_violent)\n\nLeast Violent Shootings 2014 - 2021:\n\nleast_violent <- filter(gun_violence, injured < 3, killed < 3)\n\nview(least_violent)\n\nThe “most_violent” variable only contains six incidents of gun violence. Keep in mind, this variable contains the incidents with both more than ten injured and more than ten killed. The same is true for the “least_violent” variable in that it includes both columns less than three.\nThe most violent shootings from 2014 to 2021 are as follows: The El Paso shooting, Parkland, Sutherland Springs church shooting, Las Vegas shooting, the Orlando nightclub shooting, and San Bernardino.\nAs I mentioned previously, the columns “incident_id” and “address” were not useful in my research. Below I display getting rid of them to focus purely on the remaining five columns.\n\ngun_violence$incident_id <- NULL\n\nview(gun_violence)\n\ngun_violence$address <- NULL\n\nview(gun_violence)\n\nWith the unnecessary variables gone, I can view gun_violence much quicker, only looking at the columns I need.\nFor some states, I decided to create specific variables to quickly call up certain states that I was interested in. I will also use this if I want to call up specific dates or cities in that specific state.\n\nflorida_shootings <- filter(gun_violence, state == \"Florida\")\n\nview(florida_shootings)\n\nBelow contains code for separating all shooting incidents in each specific New York borough from 2014 to 2021 to later compare NYC gun violence in specific boroughs.\n\nnycMan_shootings <- filter(gun_violence, state == \"New York\",  city_or_county == \"New York (Manhattan)\")\n\nview(nycMan_shootings)\n\nnycBrk_shootings <- filter(gun_violence, state == \"New York\",  city_or_county == \"Brooklyn\")\n\nview(nycBrk_shootings)\n\nnycQ_shootings <- filter(gun_violence, state == \"New York\",  city_or_county == \"Queens\")\n\nview(nycQ_shootings)\n\nnycBrx_shootings <- filter(gun_violence, state == \"New York\",  city_or_county == \"Bronx\")\n\nview(nycBrx_shootings)\n\nnycSI_shootings <- filter(gun_violence, state == \"New York\",  city_or_county == \"Staten Island\")\n\nview(nycSI_shootings)\n\nVisualizing Data:\nThe best way I figured to look at gun violence is in the form of graphs and bar charts.\n\nggplot(gun_violence, aes(x = state, y = killed)) + geom_boxplot()\n\n\n\n\nEven though it is difficult to view the names of each state, it provides a clear view of a zoomed-out look at gun deaths in the United States. This histogram points out instances of extreme casualties as a result of gun violence, while also giving a clear picture of any outlying incidents of gun violence (most_violent) and the general number of incidents in a given state.\nThis graph displays the number of incidents of gun violence in both Massachusetts and Mississippi, states that lie on the opposite ends of the political ideology spectrum.\n\ngun_violence %>%\n  filter(state == \"Mississippi\" | state == \"Massachusetts\") %>%\n  ggplot(aes(state, fill = killed))+\n geom_bar()+\n  theme_classic()\n\n\n\n\nBased on the graph it’s clear, at least when looking at these two states, Mississippi has almost double the number of gun violence incidents than Massachusetts. This graph is used just to test out two states before doing any sort of analysis. Right at the beginning there is something here. California is particularly interesting because their gun control laws are some of the most restrictive in the United States, yet they have one of the highest number of gun violence incidents.\n\ngun_violence %>%\n  filter(state == \"Florida\" | state == \"Texas\" | state == \"California\" | state == \"Nevada\") %>%\n  ggplot(aes(state, fill = killed))+\n geom_bar()+\n  theme_classic()+\n  labs(x = \"States\",\n       y = \"Number of Incidents\",\n       title = \"Incidents of Gun Violence in Most Violent States\")\n\n\n\n\nIn this graph, I took all the states that fall in the “most_violent” variable made previously. To reiterate, the “most_violent” variable means the most violent shootings in the dataset. Aside from Nevada, all of the states have higher than average incidents of gun violence. A factor I think of when viewing these states next to one another is their population density. This factor is almost beyond the scope of my project, but population density is a necessary variable that must be accounted for when analyzing gun violence.\nFor example, Maine has only had three incidents of gun violence in this whole period of time, while California is well over 250. New Hampshire and Wyoming only have one incident each. In New Hampshire, this incident of gun violence did not even involve a death.\ngun_violence %>% + filter(state == “New Hampshire”) %>% + ggplot(aes(state, fill = killed))+ + geom_bar()+ + theme_classic()\nNow when looking at this, population density and gun laws are not taken into account, something out of the scope of this project.\nChicago, Illinois\nChicago is one of the most notorious cities in the gun violence debate and crime violence. I use ggplot to visualize how violent the city of Chicago is compared to the entire state of Illinois.\n\ngun_violence %>%\n    filter(state == \"Illinois\", city_or_county == \"Chicago\") %>%\n    ggplot(aes(state, fill = killed))+\n    geom_bar()+\n    theme_classic()+\nlabs(x = \"Chicago, Illinois\",\n     y = \"# of Incidents\",\n     title = \"Incidents of Gun Violence in Chicago, Illinois\")\n\n\n\n\n\ngun_violence %>%\n    filter(state == \"Illinois\") %>%\n    ggplot(aes(state, fill = killed))+\n    geom_bar()+\n    theme_classic()+\nlabs(x = \"Illinois (including Chicago)\",\n     y = \"# of Incidents\",\n     title = \"Incidents of Gun Violence in Illinois\")\n\n\n\n\nWith Illinois at one of the above average incidents of gun violence, I felt it important to solely visualize shooting incidents in Chicago for this first graph. To put into perspective, Illinois incidents of gun violence totals ~340 in the entire state. About 240 of those incidents were in Chicago. It was quite shocking to discover that 70% of incidents of gun violence in Illinois happen in Chicago. Population density is an important factor for looking at gun violence in cities versus rural areas, but 70% is still a significant amount.\nThe Five Boroughs of New York City\nAs one of the major cities in the entire world, New York City is no stranger to gun violence and crime rates, but what is the safest borough? In an area impacted by the same gun laws, are certain areas of the city more prone to gun violence than others? What difference, if any, is there in a particular borough?\n\nview(nycMan_shootings)\n\nview(nycBrk_shootings)\n\nview(nycQ_shootings)\n\nview(nycBrx_shootings)\n\nview(nycSI_shootings)\n\nTo refresh, here are the various variables for each borough of NYC above. Even by looking at the numbers of the data in table form, I can already get my answer, but I find it much easier to look at in graph form.\n\ngun_violence %>%\n  filter(state == \"New York\", city_or_county == \"Bronx\") %>%\n  ggplot(aes(state, fill = killed))+\n geom_bar()+\n  theme_classic()+\n  labs(x = \"Bronx\",\n       y = \"# of Incidents\")\n\n\n\n\n\ngun_violence %>%\n  filter(state == \"New York\", city_or_county == \"Brooklyn\") %>%\n  ggplot(aes(state, fill = killed))+\n geom_bar()+\n  theme_classic()+\n  labs(x = \"Brooklyn\",\n       y = \"# of Incidents\")\n\n\n\n\n\ngun_violence %>%\n  filter(state == \"New York\", city_or_county == \"New York (Manhattan)\") %>%\n  ggplot(aes(state, fill = killed))+\n geom_bar()+\n  theme_classic()+\n  labs(x = \"Manhattan\",\n       y = \"# of Incidents\")\n\n\n\n\n\ngun_violence %>%\n  filter(state == \"New York\", city_or_county == \"Queens\") %>%\n  ggplot(aes(state, fill = killed))+\n geom_bar()+\n  theme_classic()+\n  labs(x = \"Queens\",\n       y = \"# of Incidents\")\n\n\n\n\nI chose not to include the graph for Staten Island because there are no recorded incidents of gun violence in this dataset, technically making it the “safest” borough in New York City. Brooklyn far surpasses every other borough with a total of 45, which was surprising to me until I researched the total population of the boroughs. Brooklyn also has the largest population, so the elevated number of incidents makes sense in that regard. At first, I thought Manhattan had the largest population, but they fall at number three according to 2016 census data. With the same data, Staten Island has the lowest population, explaining the low number of incidents of gun violence. In this case, it appears population is mostly the factor for the high number of gun violence incidents within the boroughs. It’s a state with the same gun laws, yet we are clearly observing a huge difference within the different regions.\nReflections:\nThis project was one of the most fascinating I have taken on. While it took me some time to settle in on this data set, I’m glad I did. Never had I looked into gun violence from a pure numbers standpoint and a lot of the results I came out with, in just seven years of data. It was extremely rewarding to look at gun violence in America from this point of view because, as policy can be debated ad nauseum, you really will not be able to understand this issue without looking at the data. I realize that is true for many issues that plague our political atmosphere, and even though it will not make you an expert, it can still provide an insight that was previously inaccessible.\nThere is much more that could be done with this project, as the scope is massive. My novice abilities in R have to be one of the limitations because I know there are other ways I can manipulate and visualize the trends in this dataset, but I am unable to do so. What I chose to look at is only a fraction of the ways to look at the data. Had I been able to dedicate more time to such a project, I would compare more states based on political leanings and their incidents of gun violence. As I have mentioned a few times, the importance of population density cannot be overlooked. I addressed it mildly, but I would have liked to use it so much more in this project.\nConclusion:\nI wish I was able to find/have more time with this dataset to further parse it out. As a first project, this was extremely interesting, yet grueling. Luckily, the data cleaning process was not too difficult, but there was a lot to visualize and I envision, with more time, cleaning up graphs and creating more variables for individual states. I greatly wish I was able to get into more analysis of states in a particular region and incorporate the population density. I do not believe the scope of the project was too much to handle for someone with little experience in research and data analysis with R. \n\nThe dataset was a perfect amount to handle, but I do desire to use one that has incidents of gun violence going back much further. This was purely a snapshot of only seven years, but a lot can be seen with a dataset as extensive as this one and I think the extent of my project showed that. \nBibliography:\nhttps://www.kaggle.com/konivat/us-gun-violence-archive-2014\nhttps://www.rosenfeldinjurylawyers.com/news/safest-dangerous-neighborhoods-chicago/"
  },
  {
    "objectID": "posts/Blog_post_6.html",
    "href": "posts/Blog_post_6.html",
    "title": "Blog Post 6",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Blog_post_6.html#notes-from-other-research",
    "href": "posts/Blog_post_6.html#notes-from-other-research",
    "title": "Blog Post 6",
    "section": "Notes from other research",
    "text": "Notes from other research"
  },
  {
    "objectID": "posts/Blog_post_6.html#naive-bayes",
    "href": "posts/Blog_post_6.html#naive-bayes",
    "title": "Blog Post 6",
    "section": "Naive Bayes",
    "text": "Naive Bayes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quinn_he_blog",
    "section": "",
    "text": "Final\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 6\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuinn\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuinn\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nQuinn He\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a final project I completed for Intro to Data Science.\n\n\n\n\n\n\nJan 9, 2022\n\n\nQuinn He\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a graduate student in the Data Analytics and Computational Social Science M.S. program at the University of Massachusetts Amherst. My research interests include studying the flow of misinformation on social media, data visualization, and digital behavioral data. I earned my B.A. in English with a certificate in the Study and Practice of Writing, as well as a minor in Psychology from UMass Amherst. My plan for a career is to combine what I’ve learned in these two unique disciplines and act as a communicator of data for companies."
  }
]